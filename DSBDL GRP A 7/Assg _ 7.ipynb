{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebeb0d9-2ffd-430b-83ec-dcd8d79171bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96266e29-79c3-49d9-98f0-22669b6ef448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871e1b14-ed39-46f6-9e47-1f6b98b364c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34534b2-5ce2-4279-8d3d-b90b37e4bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Original Document:\n",
      " Natural Language Processing (NLP) enables machines to understand and interpret human language. It bridges the gap between computers and humans by analyzing, tokenizing, and processing textual data. Techniques like stemming and lemmatization play a crucial role in simplifying text data while retaining its meaning. NLP is widely used in applications such as chatbots, sentiment analysis, and search engines.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”¹ Original Document:\\n\", document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c882069a-6f94-4a22-bbac-0a5f1a91723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Tokenized Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'enables', 'machines', 'to', 'understand', 'and', 'interpret', 'human', 'language', '.', 'It', 'bridges', 'the', 'gap', 'between', 'computers', 'and', 'humans', 'by', 'analyzing', ',', 'tokenizing', ',', 'and', 'processing', 'textual', 'data', '.', 'Techniques', 'like', 'stemming', 'and', 'lemmatization', 'play', 'a', 'crucial', 'role', 'in', 'simplifying', 'text', 'data', 'while', 'retaining', 'its', 'meaning', '.', 'NLP', 'is', 'widely', 'used', 'in', 'applications', 'such', 'as', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'search', 'engines', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(document)\n",
    "print(\"\\nðŸ”¹ Tokenized Words:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86c7303f-4603-4ee7-8775-c0e0bdc446b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('interpret', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), ('It', 'PRP'), ('bridges', 'VBZ'), ('the', 'DT'), ('gap', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('by', 'IN'), ('analyzing', 'VBG'), (',', ','), ('tokenizing', 'VBG'), (',', ','), ('and', 'CC'), ('processing', 'VBG'), ('textual', 'JJ'), ('data', 'NNS'), ('.', '.'), ('Techniques', 'NNS'), ('like', 'IN'), ('stemming', 'VBG'), ('and', 'CC'), ('lemmatization', 'NN'), ('play', 'VBP'), ('a', 'DT'), ('crucial', 'JJ'), ('role', 'NN'), ('in', 'IN'), ('simplifying', 'VBG'), ('text', 'JJ'), ('data', 'NNS'), ('while', 'IN'), ('retaining', 'VBG'), ('its', 'PRP$'), ('meaning', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('is', 'VBZ'), ('widely', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('applications', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('chatbots', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('and', 'CC'), ('search', 'NN'), ('engines', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nðŸ”¹ POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2882c948-e8c3-4da2-b27e-0752fdda83cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ After Stop Words Removal: ['Natural', 'Language', 'Processing', 'NLP', 'enables', 'machines', 'understand', 'interpret', 'human', 'language', 'bridges', 'gap', 'computers', 'humans', 'analyzing', 'tokenizing', 'processing', 'textual', 'data', 'Techniques', 'like', 'stemming', 'lemmatization', 'play', 'crucial', 'role', 'simplifying', 'text', 'data', 'retaining', 'meaning', 'NLP', 'widely', 'used', 'applications', 'chatbots', 'sentiment', 'analysis', 'search', 'engines']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"\\nðŸ”¹ After Stop Words Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd86a66-7311-4b9c-bee6-5f5cd3f3fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ After Stemming: ['natur', 'languag', 'process', 'nlp', 'enabl', 'machin', 'understand', 'interpret', 'human', 'languag', 'bridg', 'gap', 'comput', 'human', 'analyz', 'token', 'process', 'textual', 'data', 'techniqu', 'like', 'stem', 'lemmat', 'play', 'crucial', 'role', 'simplifi', 'text', 'data', 'retain', 'mean', 'nlp', 'wide', 'use', 'applic', 'chatbot', 'sentiment', 'analysi', 'search', 'engin']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nðŸ”¹ After Stemming:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c858f76-b033-4378-8113-0b234a8a6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ After Lemmatization: ['Natural', 'Language', 'Processing', 'NLP', 'enables', 'machine', 'understand', 'interpret', 'human', 'language', 'bridge', 'gap', 'computer', 'human', 'analyzing', 'tokenizing', 'processing', 'textual', 'data', 'Techniques', 'like', 'stemming', 'lemmatization', 'play', 'crucial', 'role', 'simplifying', 'text', 'data', 'retaining', 'meaning', 'NLP', 'widely', 'used', 'application', 'chatbots', 'sentiment', 'analysis', 'search', 'engine']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nðŸ”¹ After Lemmatization:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255007e0-53f2-43f3-90d0-2d77b49707fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [document]  # If `text.txt` contains multiple documents, split them into a list\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5687bd1f-9a24-46f0-8ffb-c7068bbf3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ TF-IDF Representation:\n",
      "    analysis  analyzing       and  applications        as   between   bridges  \\\n",
      "0  0.107211   0.107211  0.536056      0.107211  0.107211  0.107211  0.107211   \n",
      "\n",
      "         by  chatbots  computers  ...  techniques      text   textual  \\\n",
      "0  0.107211  0.107211   0.107211  ...    0.107211  0.107211  0.107211   \n",
      "\n",
      "        the        to  tokenizing  understand      used     while    widely  \n",
      "0  0.107211  0.107211    0.107211    0.107211  0.107211  0.107211  0.107211  \n",
      "\n",
      "[1 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nðŸ”¹ TF-IDF Representation:\\n\", df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5483c-ebb6-4a14-9904-1ea7b9874e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
